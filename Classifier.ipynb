{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tanguyvans/StGeorge/blob/main/Classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports "
      ],
      "metadata": {
        "id": "Q5YC_QVWkDeK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LxnjY0YEKtkZ"
      },
      "outputs": [],
      "source": [
        "!/opt/bin/nvidia-smi\n",
        "!rm -rf sample_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "p-M9BBXnSTwt"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import urllib\n",
        "import os\n",
        "from matplotlib import pyplot as plt\n",
        "import math\n",
        "\n",
        "import zipfile\n",
        "import pandas as pd\n",
        "import csv\n",
        "from google.colab import drive\n",
        "from google.colab import files\n",
        "\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "# Deep Learning\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras.preprocessing import image\n",
        "from keras.models import Model, load_model\n",
        "from keras import backend as K\n",
        "from keras.applications.vgg16 import VGG16, preprocess_input # 224*224\n",
        "from keras.applications.xception import Xception, preprocess_input, decode_predictions # 299*299\n",
        "from keras.applications.mobilenet import MobileNet, preprocess_input, decode_predictions # 224*224\n",
        "from keras.applications.densenet import DenseNet121 # 224*224\n",
        "\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.losses import categorical_crossentropy\n",
        "from keras.layers import Dense, GlobalAveragePooling2D, Activation, Flatten, Dropout\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Defining the configuration"
      ],
      "metadata": {
        "id": "hoFdxzSxsxG7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/gdrive')\n",
        "!printf '%s\\n' 'george' 'no_george'> classes.txt\n",
        "\n",
        "source_path = '/content/gdrive/MyDrive/test_assignment_cv/'\n",
        "dataset_path = '/content/gdrive/MyDrive/george_ds/'\n",
        "test_path= '/content/gdrive/MyDrive/george_test_ds/'"
      ],
      "metadata": {
        "id": "9o93SxsXUp0q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "IBeGWQmvTH4X"
      },
      "outputs": [],
      "source": [
        "configs = dict(\n",
        "    nb_classes = 2,\n",
        "    batch_size = 64,\n",
        "    input_dim = 224, \n",
        "    epochs = 15, \n",
        "    dataset_name = dataset_path,\n",
        "    test_name = test_path,\n",
        "    classifier = \"Xception\",\n",
        "    pretrain_weights = 'imagenet',\n",
        "    init_learning_rate = 0.001,\n",
        "    lr_decay_rate = 0.1, \n",
        "    optimizer = 'adam',\n",
        "    loss_fn = 'categorical_crossentropy',\n",
        "    metrics = ['acc'],\n",
        "    seed = 42, \n",
        "    validation_split = 0.2\n",
        ")\n",
        "\n",
        "classes_path = 'classes.txt'\n",
        "log_path='logs'\n",
        "result_path = 'results/' + configs['classifier']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b4va693XLoq7"
      },
      "outputs": [],
      "source": [
        "with open(classes_path, 'r') as f:\n",
        "    classes = f.readlines()\n",
        "    classes = list(map(lambda x: x.strip(), classes))\n",
        "num_classes = len(classes)\n",
        "\n",
        "print(f'Classes : {classes}')\n",
        "print(f'Number of classes : {num_classes}')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating Train, validation adn test dataset"
      ],
      "metadata": {
        "id": "zlXQaaWYthqP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EYtc5DCYLmR8"
      },
      "outputs": [],
      "source": [
        "train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "\tconfigs['dataset_name'],          \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t  # Path of the dataset\n",
        "\tvalidation_split = configs['validation_split'],             \t\t\t\t\t\t# Data division : validation (20%), train (80%)\n",
        "\tsubset = 'training',                \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t# Selection of training data\n",
        "\tseed = configs['seed'],                          \t\t\t\t\t\t\t\t\t\t\t\t# Initialization of random generator (for permutations)\n",
        "\timage_size = (configs['input_dim'], configs['input_dim']),    \t\t\t\t\t# Input size of images\n",
        "\tbatch_size = configs['batch_size'],\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t# Batch_size\n",
        "  label_mode = 'categorical'     \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t# Conversion to One-Hot format\n",
        ")\n",
        "\n",
        "val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "\tconfigs['dataset_name'],          \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t  # Path of the dataset\n",
        "\tvalidation_split = configs['validation_split'],             \t\t\t\t\t\t# Data division : validation (20%), train (80%)\n",
        "\tsubset = 'validation',              \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t# Selection of validation data\n",
        "\tseed = configs['seed'],                         \t\t\t\t\t\t\t\t\t\t\t\t# Initialization of random generator (for permutations)\n",
        "\timage_size = (configs['input_dim'], configs['input_dim']),    \t\t\t\t\t# Input size of images\n",
        "\tbatch_size = configs['batch_size'], \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t# Batch_size\n",
        "  label_mode = 'categorical'     \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t# Conversion to One-Hot format\n",
        ")\n",
        "\n",
        "test_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "\tconfigs['test_name'],          \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t  \n",
        "\timage_size = (configs['input_dim'], configs['input_dim']),    \t\t\t\t\t# Input size of images\n",
        "  label_mode = 'categorical'     \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t# Conversion to One-Hot format\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Making data augmentation on the training set"
      ],
      "metadata": {
        "id": "laez9ujftnJg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VH7IMx_4Lm3t"
      },
      "outputs": [],
      "source": [
        "data_gen_args = dict(              \n",
        "    featurewise_center=False,\n",
        "    featurewise_std_normalization=False,\n",
        "    rotation_range=20, \n",
        "    width_shift_range=0.3, \n",
        "    height_shift_range=0.3,\n",
        "    validation_split = 0.2,\n",
        "    horizontal_flip = True,\n",
        "    dtype = 'uint8'\n",
        ")\n",
        "\n",
        "color_datagen = ImageDataGenerator(**data_gen_args)\n",
        "\n",
        "train_generator = color_datagen.flow_from_directory(\n",
        "  configs['dataset_name'],          \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t  # Path of the dataset             \t\t\t\n",
        "\tsubset = 'training',                \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t# Selection of training data\n",
        "\tseed = configs['seed'],                         \t\t\t\t\t\t\t\t\t\t\t\t# Initialization of random generator (for permutations)\n",
        "\ttarget_size = (configs['input_dim'], configs['input_dim']),    \t\t\t\t\t# Input size of images\n",
        "\tbatch_size = configs['batch_size'], \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t# Batch_size\n",
        "  class_mode = 'categorical',\n",
        "  shuffle = True\n",
        "  )"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Building the model"
      ],
      "metadata": {
        "id": "BB_7BqDftwTc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "Rpx38scQQZ_I"
      },
      "outputs": [],
      "source": [
        "def build_model():\n",
        "\n",
        "  base_model = Xception(\n",
        "      include_top = False, \n",
        "      weights = 'imagenet', \n",
        "      input_shape = (configs['input_dim'], configs['input_dim'],3))\n",
        "\n",
        "  model = base_model.output\n",
        "  model = Flatten()(model)\n",
        "\n",
        "  # Adding new layers to the xception model\n",
        "  model = Dense(128, activation='relu')(model)\n",
        "  model = Dropout(0.4)(model)\n",
        "  model = Dense(32, activation = 'relu')(model)\n",
        "  model = Dropout(0.4)(model)\n",
        "  predictions = Dense(2, activation = 'softmax')(model)\n",
        "\n",
        "  model = Model(inputs=base_model.inputs, outputs = predictions)\n",
        "\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = build_model()"
      ],
      "metadata": {
        "id": "pP1yrggoZ4XQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# training the model"
      ],
      "metadata": {
        "id": "9X5b64UduxFn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(config: dict, callbacks: list, verbose: int=0):\n",
        "  tf.keras.backend.clear_session()                  \n",
        "  model = build_model()\n",
        "\n",
        "  # Freasing all the layers (can't update the weights)\n",
        "  for layer in model.layers:\n",
        "      layer.trainable = False       \n",
        "  \n",
        "  # unfreasing last layers to update the weihgts\n",
        "  for layer in model.layers[:-6]:\n",
        "      layer.trainable = True\n",
        "\n",
        "  # choosing the optimizer usually Adam is the best\n",
        "  opt = keras.optimizers.SGD(learning_rate = config['init_learning_rate'])\n",
        "  opt2 = keras.optimizers.Adam(learning_rate = config['init_learning_rate'])\n",
        "  opt3 = keras.optimizers.RMSprop(learning_rate = config['init_learning_rate'])\n",
        "\n",
        "  model.compile(loss = config['loss_fn'],\n",
        "                optimizer = opt2,\n",
        "                metrics = config['metrics'])  \n",
        "\n",
        "  if os.path.exists(result_path) == False:\n",
        "      os.makedirs(result_path)\n",
        "\n",
        "  history = model.fit_generator(\n",
        "      train_generator,\n",
        "      steps_per_epoch=math.ceil(len(train_generator)),\n",
        "      epochs=config['epochs'],\n",
        "      validation_data = val_ds,\n",
        "      validation_steps=math.ceil(len(val_ds)),\n",
        "      callbacks = callbacks\n",
        "  )\n",
        "  \n",
        "  return model, history"
      ],
      "metadata": {
        "id": "JIix5kwYaI2c"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir = log_path,\n",
        "                                                      histogram_freq = 1)\n",
        "\n",
        "# earlystopping to avoid overfitting. kepping the best model\n",
        "stopper_callback = EarlyStopping(monitor = 'val_loss',\n",
        "                                 patience = 5,\n",
        "                                 mode='auto',\n",
        "                                 restore_best_weights=True)\n",
        "\n",
        "# saving model at chekcpoints\n",
        "ckpt_save = os.path.join('.', 'model_fine_ep_{epoch}_val_acc_{val_acc:.3f}.h5')\n",
        "ckpt_callbak = ModelCheckpoint(ckpt_save,\n",
        "                               monitor = 'val_acc',\n",
        "                               verbose = 1,\n",
        "                               save_best_only = True,\n",
        "                               mode = 'auto')\n",
        "\n"
      ],
      "metadata": {
        "id": "htDus0dEaNMm"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model evaluation"
      ],
      "metadata": {
        "id": "b-ZuJwQSuzv3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "callbacks = [tensorboard_callback,\n",
        "             stopper_callback,\n",
        "             ckpt_callbak]\n",
        "\n",
        "# Start training\n",
        "model, history = train(configs, callbacks, 1)\n",
        "\n",
        "# evaluate the model on the validation data\n",
        "loss, acc = model.evaluate(val_ds)\n",
        "print(f'Validation loss: {loss}, validation accuracy : {acc}')\n",
        "\n",
        "# evaluate the mode lon the test data\n",
        "loss, acc = model.evaluate(test_ds)\n",
        "print(f'Validation loss: {loss}, validation accuracy : {acc}')\n",
        "\n",
        "# saving the model\n",
        "model.save('Xception_64_15.h5')"
      ],
      "metadata": {
        "id": "nAKt-GNlaWMp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Making a prediction"
      ],
      "metadata": {
        "id": "Wvq-j9xzuvtR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = load_model('Xception_64_15.h5')\n",
        "\n",
        "# upload an image that we want to classify\n",
        "files.upload()"
      ],
      "metadata": {
        "id": "DRZfzv3_VWbo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classes = train_ds.class_names\n",
        "image_path = \"/content/img.jpg\"\n",
        "img = Image.open(image_path).convert('RGB')\n",
        "x = tf.keras.utils.img_to_array(img,data_format='channels_last')\n",
        "x = tf.keras.preprocessing.image.smart_resize(x, size=(configs['input_dim'], configs['input_dim']))\n",
        "x = np.expand_dims(x, axis=0)\n",
        "\n",
        "# predict\n",
        "pred = model.predict(x,batch_size=1)[0]\n",
        "\n",
        "\n",
        "print(\"THE prediction is: \",classes[np.argmax(pred)])"
      ],
      "metadata": {
        "id": "rJvRmE6CV5Cr"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNMIN1ThGO1P5dL7+4Vr66B",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}